import mindspore as ms
import os

# ================= é…ç½®åŒº =================
INPUT_CKPT = "/root/LLM/mindspore_model.ckpt"
# æœ€ç»ˆä¿®æ­£ç‰ˆ ckpt
OUTPUT_CKPT = "/root/LLM/mindspore_model_final.ckpt"


# =========================================

def patch():
    print(f">>> æ­£åœ¨è¯»å–: {INPUT_CKPT}")
    if not os.path.exists(INPUT_CKPT):
        print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
        return

    param_dict = ms.load_checkpoint(INPUT_CKPT)
    new_params = []

    # æ ‡è®°æ˜¯å¦æ‰¾åˆ°äº† embeddingï¼Œä»¥ä¾¿å¤åˆ¶ç»™ lm_head
    embed_tensor = None
    has_lm_head = False
    has_norm_out = False

    print(">>> æ­£åœ¨åº”ç”¨è¡¥ä¸...")
    for name, tensor in param_dict.items():
        # 1. ä¿®å¤ Norm å±‚åå­—
        if name == "model.norm.weight":
            print(f"   ğŸ› ï¸ ä¿®å¤: {name} -> model.norm_out.weight")
            name = "model.norm_out.weight"
            has_norm_out = True
        elif name == "model.norm_out.weight":
            has_norm_out = True

        # è®°å½• Embedding ç”¨äºå…‹éš†
        if name == "model.tok_embeddings.embedding_weight":
            embed_tensor = tensor

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ lm_head
        if name == "lm_head.weight":
            has_lm_head = True

        new_params.append({"name": name, "data": tensor})

    # 2. ä¿®å¤ LM Head (å¦‚æœç¼ºå¤±ï¼Œä» Embedding å…‹éš†)
    if not has_lm_head:
        if embed_tensor is not None:
            print("   ğŸ› ï¸ ä¿®å¤: ç¼ºå¤± lm_head.weightï¼Œæ­£åœ¨ä» Embedding å…‹éš†...")
            new_params.append({"name": "lm_head.weight", "data": embed_tensor})
        else:
            print("âŒ ä¸¥é‡é”™è¯¯: æ²¡æ‰¾åˆ° Embedding å±‚ï¼Œæ— æ³•å…‹éš† lm_headï¼")

    if not has_norm_out:
        print("âš ï¸ è­¦å‘Š: æ²¡æ‰¾åˆ° model.norm.weightï¼Œå¯èƒ½åå­—ä¸å¯¹ï¼Ÿ")

    print(f">>> æ­£åœ¨ä¿å­˜æœ€ç»ˆç‰ˆ: {OUTPUT_CKPT}")
    ms.save_checkpoint(new_params, OUTPUT_CKPT)
    print("âœ… è¡¥ä¸å®Œæˆï¼")


if __name__ == "__main__":
    patch()